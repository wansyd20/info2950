{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c57c82e5",
   "metadata": {},
   "source": [
    "Research Questions Notes: \n",
    "would we have enough vine results? compare against vine reulsts, scale against unhelpful, not helpful, fake\n",
    "would also have to come up with why vine vs nonvine is an important research question\n",
    "\n",
    "Readme File:\n",
    "create a readme, don't submit all the separate files to CMS just to github and creat a readme about what is happening\n",
    "very tough to look for fake reviews, is there or is there not fake reviews??"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f57e5b",
   "metadata": {},
   "source": [
    "Meeting with Professor:\n",
    " - project could lean heavily towards text analysis\n",
    "     - major challenge would be --> how do i turn this block of text into numerical features? \n",
    "         - start by counting individual distinct words\n",
    "     - just looking at factors (e.g. verified purchase, length of review, prescense/absence of title) will be enough for phase 2\n",
    "     - if struggling --> have a conversation w/ professor again about implementing text analysis\n",
    "         - **professor could give us a different dataset and compare two different review datasets to see rating pracgices between different online communities, why are there distinctive differences, predict rating of product on another marketplace that has diff marketplace standard**\n",
    " - looking for relationships between data, how we can build a model to proedict other target vairables of interest, \n",
    " - research question: \n",
    "     - finding fake reviews\n",
    "     - (as first step, exploratory analysis) characterizing different product categories by distribution of review ratings, types\n",
    "     \n",
    "using vine reviews as a gold standard while subsetting 10,000 reviews, we create can two distinct datasets two prevent loss of vine reviews:\n",
    " - take all the vine reviews (careful not to amplify by making sure to distinguish it as a vine review) \n",
    " - second dataset of non vine reviews that is randomly sampled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b29dce2",
   "metadata": {},
   "source": [
    "# Research Question\n",
    "1. Are Amazon Beauty product reviews an accurate representation of how well a brand is doing in the greater market? Are we able to determine general consumer sentiment about a brand based on just Amazon reviews?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1238ec41",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4313344a",
   "metadata": {},
   "source": [
    "# Data Collection and Cleaning of Amazon Review Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9a3955fe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 10093: expected 15 fields, saw 22\\nSkipping line 31965: expected 15 fields, saw 22\\nSkipping line 49886: expected 15 fields, saw 22\\nSkipping line 49905: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 67579: expected 15 fields, saw 22\\nSkipping line 75367: expected 15 fields, saw 22\\nSkipping line 92462: expected 15 fields, saw 22\\nSkipping line 105041: expected 15 fields, saw 22\\nSkipping line 109697: expected 15 fields, saw 22\\nSkipping line 121931: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 139492: expected 15 fields, saw 22\\nSkipping line 158729: expected 15 fields, saw 22\\nSkipping line 165784: expected 15 fields, saw 22\\nSkipping line 176996: expected 15 fields, saw 22\\nSkipping line 182928: expected 15 fields, saw 22\\nSkipping line 195841: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 196938: expected 15 fields, saw 22\\nSkipping line 202535: expected 15 fields, saw 22\\nSkipping line 261147: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 265777: expected 15 fields, saw 22\\nSkipping line 277693: expected 15 fields, saw 22\\nSkipping line 280010: expected 15 fields, saw 22\\nSkipping line 296315: expected 15 fields, saw 22\\nSkipping line 299043: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 334564: expected 15 fields, saw 22\\nSkipping line 337801: expected 15 fields, saw 22\\nSkipping line 341391: expected 15 fields, saw 22\\nSkipping line 354940: expected 15 fields, saw 22\\nSkipping line 366330: expected 15 fields, saw 22\\nSkipping line 367649: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 399174: expected 15 fields, saw 22\\nSkipping line 414439: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 473579: expected 15 fields, saw 22\\nSkipping line 483540: expected 15 fields, saw 22\\nSkipping line 499744: expected 15 fields, saw 22\\nSkipping line 505775: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 547693: expected 15 fields, saw 22\\nSkipping line 561254: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 609329: expected 15 fields, saw 22\\nSkipping line 642814: expected 15 fields, saw 22\\nSkipping line 643189: expected 15 fields, saw 22\\nSkipping line 647075: expected 15 fields, saw 22\\nSkipping line 647457: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 660868: expected 15 fields, saw 22\\nSkipping line 668514: expected 15 fields, saw 22\\nSkipping line 673314: expected 15 fields, saw 22\\nSkipping line 700416: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 723492: expected 15 fields, saw 22\\nSkipping line 725052: expected 15 fields, saw 22\\nSkipping line 726222: expected 15 fields, saw 22\\nSkipping line 744078: expected 15 fields, saw 22\\nSkipping line 753129: expected 15 fields, saw 22\\nSkipping line 758347: expected 15 fields, saw 22\\nSkipping line 759076: expected 15 fields, saw 22\\nSkipping line 759139: expected 15 fields, saw 22\\nSkipping line 768106: expected 15 fields, saw 22\\nSkipping line 777835: expected 15 fields, saw 22\\nSkipping line 779763: expected 15 fields, saw 22\\nSkipping line 781395: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 787023: expected 15 fields, saw 22\\nSkipping line 811679: expected 15 fields, saw 22\\nSkipping line 811739: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 855784: expected 15 fields, saw 22\\nSkipping line 878325: expected 15 fields, saw 22\\nSkipping line 886822: expected 15 fields, saw 22\\nSkipping line 890742: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 919607: expected 15 fields, saw 22\\nSkipping line 920655: expected 15 fields, saw 22\\nSkipping line 923107: expected 15 fields, saw 22\\nSkipping line 930890: expected 15 fields, saw 22\\nSkipping line 932841: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1003214: expected 15 fields, saw 22\\nSkipping line 1007588: expected 15 fields, saw 22\\nSkipping line 1018374: expected 15 fields, saw 22\\nSkipping line 1022909: expected 15 fields, saw 22\\nSkipping line 1030983: expected 15 fields, saw 22\\nSkipping line 1048441: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1056292: expected 15 fields, saw 22\\nSkipping line 1056518: expected 15 fields, saw 22\\nSkipping line 1073064: expected 15 fields, saw 22\\nSkipping line 1088887: expected 15 fields, saw 22\\nSkipping line 1103881: expected 15 fields, saw 22\\nSkipping line 1111021: expected 15 fields, saw 22\\nSkipping line 1111314: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1119421: expected 15 fields, saw 22\\nSkipping line 1119549: expected 15 fields, saw 22\\nSkipping line 1130122: expected 15 fields, saw 22\\nSkipping line 1132767: expected 15 fields, saw 22\\nSkipping line 1143315: expected 15 fields, saw 22\\nSkipping line 1151947: expected 15 fields, saw 22\\nSkipping line 1154207: expected 15 fields, saw 22\\nSkipping line 1154616: expected 15 fields, saw 22\\nSkipping line 1155875: expected 15 fields, saw 22\\nSkipping line 1164714: expected 15 fields, saw 22\\nSkipping line 1164959: expected 15 fields, saw 22\\nSkipping line 1169410: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1184604: expected 15 fields, saw 22\\nSkipping line 1203964: expected 15 fields, saw 22\\nSkipping line 1211287: expected 15 fields, saw 22\\nSkipping line 1217834: expected 15 fields, saw 22\\nSkipping line 1235346: expected 15 fields, saw 22\\nSkipping line 1238073: expected 15 fields, saw 22\\nSkipping line 1238439: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1246837: expected 15 fields, saw 22\\nSkipping line 1263235: expected 15 fields, saw 22\\nSkipping line 1265620: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1312400: expected 15 fields, saw 22\\nSkipping line 1314122: expected 15 fields, saw 22\\nSkipping line 1319707: expected 15 fields, saw 22\\nSkipping line 1337672: expected 15 fields, saw 22\\nSkipping line 1343961: expected 15 fields, saw 22\\nSkipping line 1346372: expected 15 fields, saw 22\\nSkipping line 1358447: expected 15 fields, saw 22\\nSkipping line 1370844: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1406108: expected 15 fields, saw 22\\nSkipping line 1435069: expected 15 fields, saw 22\\nSkipping line 1439866: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1442123: expected 15 fields, saw 22\\nSkipping line 1463237: expected 15 fields, saw 22\\nSkipping line 1469027: expected 15 fields, saw 22\\nSkipping line 1469598: expected 15 fields, saw 22\\nSkipping line 1482636: expected 15 fields, saw 22\\nSkipping line 1484745: expected 15 fields, saw 22\\nSkipping line 1499831: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1508882: expected 15 fields, saw 22\\nSkipping line 1514887: expected 15 fields, saw 22\\nSkipping line 1527564: expected 15 fields, saw 22\\nSkipping line 1569519: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1583105: expected 15 fields, saw 22\\nSkipping line 1604380: expected 15 fields, saw 22\\nSkipping line 1607380: expected 15 fields, saw 22\\nSkipping line 1631601: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1642095: expected 15 fields, saw 22\\nSkipping line 1646714: expected 15 fields, saw 22\\nSkipping line 1655248: expected 15 fields, saw 22\\nSkipping line 1657807: expected 15 fields, saw 22\\nSkipping line 1667534: expected 15 fields, saw 22\\nSkipping line 1668489: expected 15 fields, saw 22\\nSkipping line 1691733: expected 15 fields, saw 22\\nSkipping line 1701102: expected 15 fields, saw 22\\nSkipping line 1701499: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1704450: expected 15 fields, saw 22\\nSkipping line 1706154: expected 15 fields, saw 22\\nSkipping line 1712789: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1773984: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1846441: expected 15 fields, saw 22\\nSkipping line 1848019: expected 15 fields, saw 22\\nSkipping line 1856015: expected 15 fields, saw 22\\nSkipping line 1858248: expected 15 fields, saw 22\\nSkipping line 1859629: expected 15 fields, saw 22\\nSkipping line 1873117: expected 15 fields, saw 22\\nSkipping line 1894414: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 1902421: expected 15 fields, saw 22\\nSkipping line 1909201: expected 15 fields, saw 22\\nSkipping line 1914394: expected 15 fields, saw 22\\nSkipping line 1936976: expected 15 fields, saw 22\\nSkipping line 1940327: expected 15 fields, saw 22\\nSkipping line 1945664: expected 15 fields, saw 22\\nSkipping line 1946171: expected 15 fields, saw 22\\nSkipping line 1946284: expected 15 fields, saw 22\\nSkipping line 1946835: expected 15 fields, saw 22\\nSkipping line 1952446: expected 15 fields, saw 22\\nSkipping line 1953387: expected 15 fields, saw 22\\n'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "b'Skipping line 1979093: expected 15 fields, saw 22\\nSkipping line 1982997: expected 15 fields, saw 22\\nSkipping line 1992924: expected 15 fields, saw 22\\nSkipping line 1996161: expected 15 fields, saw 22\\nSkipping line 2003175: expected 15 fields, saw 22\\nSkipping line 2024153: expected 15 fields, saw 22\\nSkipping line 2026345: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2041159: expected 15 fields, saw 22\\nSkipping line 2042954: expected 15 fields, saw 22\\nSkipping line 2044244: expected 15 fields, saw 22\\nSkipping line 2047949: expected 15 fields, saw 22\\nSkipping line 2051022: expected 15 fields, saw 22\\nSkipping line 2052365: expected 15 fields, saw 22\\nSkipping line 2064460: expected 15 fields, saw 22\\nSkipping line 2077010: expected 15 fields, saw 22\\nSkipping line 2083893: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2097514: expected 15 fields, saw 22\\nSkipping line 2100479: expected 15 fields, saw 22\\nSkipping line 2103183: expected 15 fields, saw 22\\nSkipping line 2108608: expected 15 fields, saw 22\\nSkipping line 2116577: expected 15 fields, saw 22\\nSkipping line 2127375: expected 15 fields, saw 22\\nSkipping line 2128053: expected 15 fields, saw 22\\nSkipping line 2135954: expected 15 fields, saw 22\\nSkipping line 2137154: expected 15 fields, saw 22\\nSkipping line 2140279: expected 15 fields, saw 22\\nSkipping line 2150764: expected 15 fields, saw 22\\nSkipping line 2151464: expected 15 fields, saw 22\\nSkipping line 2151588: expected 15 fields, saw 22\\nSkipping line 2157049: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2163762: expected 15 fields, saw 22\\nSkipping line 2167939: expected 15 fields, saw 22\\nSkipping line 2172050: expected 15 fields, saw 22\\nSkipping line 2177960: expected 15 fields, saw 22\\nSkipping line 2202813: expected 15 fields, saw 22\\nSkipping line 2207828: expected 15 fields, saw 22\\nSkipping line 2211189: expected 15 fields, saw 22\\nSkipping line 2211589: expected 15 fields, saw 22\\nSkipping line 2214034: expected 15 fields, saw 22\\nSkipping line 2214264: expected 15 fields, saw 22\\nSkipping line 2214462: expected 15 fields, saw 22\\nSkipping line 2215027: expected 15 fields, saw 22\\nSkipping line 2215639: expected 15 fields, saw 22\\nSkipping line 2216007: expected 15 fields, saw 22\\nSkipping line 2217132: expected 15 fields, saw 22\\nSkipping line 2226703: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2231683: expected 15 fields, saw 22\\nSkipping line 2245222: expected 15 fields, saw 22\\nSkipping line 2256136: expected 15 fields, saw 22\\nSkipping line 2269399: expected 15 fields, saw 22\\nSkipping line 2283979: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2340899: expected 15 fields, saw 22\\nSkipping line 2342134: expected 15 fields, saw 22\\nSkipping line 2342748: expected 15 fields, saw 22\\nSkipping line 2348402: expected 15 fields, saw 22\\nSkipping line 2355164: expected 15 fields, saw 22\\nSkipping line 2357020: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2366077: expected 15 fields, saw 22\\nSkipping line 2366997: expected 15 fields, saw 22\\nSkipping line 2367353: expected 15 fields, saw 22\\nSkipping line 2414691: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2464571: expected 15 fields, saw 22\\nSkipping line 2466302: expected 15 fields, saw 22\\nSkipping line 2487679: expected 15 fields, saw 22\\nSkipping line 2487771: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2506605: expected 15 fields, saw 22\\nSkipping line 2511369: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2558281: expected 15 fields, saw 22\\nSkipping line 2607202: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2625718: expected 15 fields, saw 22\\nSkipping line 2640978: expected 15 fields, saw 22\\nSkipping line 2650635: expected 15 fields, saw 22\\nSkipping line 2670724: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2690954: expected 15 fields, saw 22\\nSkipping line 2713810: expected 15 fields, saw 22\\nSkipping line 2715292: expected 15 fields, saw 22\\nSkipping line 2724453: expected 15 fields, saw 22\\nSkipping line 2724458: expected 15 fields, saw 22\\nSkipping line 2735678: expected 15 fields, saw 22\\nSkipping line 2740358: expected 15 fields, saw 22\\nSkipping line 2751188: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2763890: expected 15 fields, saw 22\\nSkipping line 2766982: expected 15 fields, saw 22\\nSkipping line 2813747: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2819306: expected 15 fields, saw 22\\nSkipping line 2883075: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 2975635: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3391761: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3474241: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3690054: expected 15 fields, saw 22\\nSkipping line 3720113: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 3763182: expected 15 fields, saw 22\\n'\n",
      "b'Skipping line 4929700: expected 15 fields, saw 22\\n'\n",
      "/Users/michellehui/opt/anaconda3/lib/python3.8/site-packages/IPython/core/interactiveshell.py:3165: DtypeWarning: Columns (7) have mixed types.Specify dtype option on import or set low_memory=False.\n",
      "  has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n"
     ]
    }
   ],
   "source": [
    "beauty_raw = pd.read_csv('/Users/michellehui/info2950_project/amazon_reviews_us_Beauty_v1_00.tsv', sep = '\\t', error_bad_lines=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b704675d",
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_raw.to_csv('amazonreviews_beauty_raw.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a4ba42b",
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty_raw = pd.read_csv('amazonreviews_beauty_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "728281c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "beauty = beauty_raw.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00cf1275",
   "metadata": {},
   "source": [
    "Here we converted the review date to datetime type. When then kept only the reviews written in 2015, which is the most recent year in the dataset. We chose 2015 so that we are able to compare against our 2015 market share data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e1f35513",
   "metadata": {},
   "outputs": [],
   "source": [
    "# keeping reviews written in 2015\n",
    "beauty['review_date'] = pd.to_datetime(beauty['review_date'])\n",
    "beauty = beauty.loc[beauty['review_date'].dt.year == 2015]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f8c9d2d",
   "metadata": {},
   "source": [
    "As Amazon sells many smaller, indie brands, we are filtering for the select corporate brands that we will be analyzing. The five leading companies we will be looking at are L'oreal, Coty, Revlon, Johnson and Johnson, and Proctor and Gamble. We are including the leading brands that they own as additional brands to filter for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "28aa1020",
   "metadata": {},
   "outputs": [],
   "source": [
    "loreal = [\"L'oreal\", 'Garnier', 'Maybelline', 'Nyx', 'Cerave']\n",
    "coty = ['Rimmel', 'Covergirl', 'Sally Hansen']\n",
    "revlon = ['Revlon', 'Almay']\n",
    "jnj = ['Clean & Clear', 'Neutrogena', 'Aveeno', 'OGX', 'Lubriderm'] \n",
    "png = ['Olay','Herbal Essences', 'Pantene', 'Head and Shoulders','Aussie']\n",
    "brands = loreal + coty + revlon + jnj\n",
    "beauty = beauty.loc[beauty['product_title'].str.contains('|'.join(brands))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d135195d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([\"L'oreal\", 'Revlon', 'Coty', 'Johnson & Johnson',\n",
       "       'Proctor & Gamble'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add columns by parent corporation\n",
    "beauty.loc[beauty['product_title'].str.contains('|'.join(loreal)), 'corporation'] = \"L'oreal\"\n",
    "beauty.loc[beauty['product_title'].str.contains('|'.join(coty)), 'corporation'] = \"Coty\"\n",
    "beauty.loc[beauty['product_title'].str.contains('|'.join(revlon)), 'corporation'] = \"Revlon\"\n",
    "beauty.loc[beauty['product_title'].str.contains('|'.join(jnj)), 'corporation'] = \"Johnson & Johnson\"\n",
    "beauty.loc[beauty['product_title'].str.contains('|'.join(png)), 'corporation'] = \"Proctor & Gamble\"\n",
    "beauty.corporation.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83381042",
   "metadata": {},
   "source": [
    "In order to narrow the dataset down to a more reasonable size, we are randomly selecting 40,000 observations. We then reset the index on each dataset to be from 0-39,999."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "918f9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "# randomly narrowing down to 40,000\n",
    "beauty = beauty.sample(n=40000).reset_index()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8062cf5",
   "metadata": {},
   "source": [
    "Here we are dropping the unecessary columns.\n",
    " - We first dropped the Marketplace column because all the products were sold in the US.\n",
    " - We also dropped the index column as we reset the index. \n",
    " - We dropped product_category as all products fall into Beauty\n",
    " - total_votes is the same as helpful_votes and is redundant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "57cd6d1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropping marketplace, index column, product category, total votes\n",
    "beauty = beauty.drop(columns= ['marketplace','index','product_category', 'total_votes'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "793c5308",
   "metadata": {},
   "outputs": [],
   "source": [
    "# downloading cleaned csv file\n",
    "beauty.to_csv('amazonreviews_beauty.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f9ef331",
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing cleaned amazon review datasets\n",
    "beauty_clean = pd.read_csv('amazonreviews_beauty.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12b7c5fa",
   "metadata": {},
   "source": [
    "# Data Description\n",
    "\n",
    "## Motivation\n",
    " - **Why was this dataset created?**\n",
    "     - This dataset was intended to facilitate public study into the properties of Amazon customer reviews including how consumers evolve, express, and evaluate Amazon products.\n",
    " \n",
    " - **Who funded the creation of the dataset?**\n",
    "     - Amazon funded and collected this Amazon Customer Reviews Product for public use. The information is publicly available on Amazon Web Services.\n",
    "\n",
    "## Composition\n",
    "  - **What are the observations (rows) and the attributes (columns)?**\n",
    "      - The observations consist of each individual review. Each review has the following column attributes: customer_id, review_id, product_id, product_title, star_rating (Scale of 1-5), helpful_votes, vine (Amazon invites select credible users to review products through their Vine program), verified_purchase, review_headline, review_body, and review_date.\n",
    "  \n",
    " - **How many instances are there in the dataset? Does it contain all possible instances or is it a sample? What processes might have influenced what data was observed and recorded and what was not?**\n",
    "     - There are a total of 40,000 observations. The original dataset with over a million reviews in the beauty category is a sample collected by Amazon. Amazon constructed this dataset hoping to represent a sample of different customer opinions and demographics. We then narrowed the dataset even further by most recent year and then by select brands leading the beauty/hair care industry. \n",
    " \n",
    " - **Where can your raw source data be found, if applicable? Provide a link to the raw data (hosted in a Cornell Google Drive or Cornell Box).**\n",
    "     - The raw source can be found on Amazon Web Services. Here is the link to raw datasets also uploaded on google drive: https://drive.google.com/drive/folders/1F1Z1ERhp18uaB9J1d-dkGYhBCbJnPyXC?usp=sharing\n",
    " \n",
    " - **Is the data self contained or does it rely on other outside websites?**\n",
    "     - The data is self-contained in a downloaded tsv file.\n",
    " \n",
    " - **Does this data relate to individuals? Can the individuals be identified? Are there any subpopulations that can be identified?**\n",
    "     - Each individual review is associated to a reviewer through customer_id. However, there is no further information about this individual and customers cannot be split off into smaller subpopulations by demographics.\n",
    " \n",
    "## Collection Process\n",
    " - **If people are involved, were they aware of the data collection and if so, what purpose did they expect the data to be used for?**\n",
    "     - The reviewers in the dataset are not aware of their reviews being used for this purpose. They gave their consent to Amazon when they published their reviews publicly on Amazon. \n",
    " \n",
    " - **What time frame was the data collected?**\n",
    "     - Amazon collected reviews starting from 1995 all the way to 2015. \n",
    " \n",
    "## Preprocessing\n",
    " - **What preprocessing was done, and how did the data come to be in the form that you are using?**\n",
    "     - We conducted filtering on certain column values by year and product brand. We then randomly selected 40,000 observations to narrow down the size of our dataset.\n",
    " \n",
    "  - **Are there are any errors, sources of noise, or redundancies in the data?**\n",
    "      - There were some observations with additional attributes/columns when parsing in the data from a csv file. We handled this by skipping over these lines and not including in the final dataset as it would not be reasonable to comb through the millions of reviews to individually handle each of these errors.\n",
    " \n",
    "## Uses\n",
    " - **Has this data been used before? If so, what for?**\n",
    "     - There are no public publishings using this dataset that could be found. However, we found a few other Amazon review analysis using other Amazon datasets. These analyses consisted mainly of review sentiment analysis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873ee59d",
   "metadata": {},
   "source": [
    "# Data Limitations\n",
    " - Amazon has a lot of other smaller, indie brands that we have excluded from the cleaned dataset. Because we are not including them, our analysis might not accurately reflect the whole market share and beauty space.\n",
    " - It could be difficult to analyze the review text content as it requires in-depth natural language processing, which is beyond the scope of what we will learn in this course. \n",
    " - By selecting only 40,000 reviews, we also risk losing a lot of important observations such as vine reviews, reviews that are voted highly helpful, and various reviews across products that could have very beneficial content."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bcf2a347",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\n",
    " ## Amazon\n",
    " Sydney:\n",
    " - histogram of star ratings by parent company vs overall distribution\n",
    "     - summary statistics: mean, median, standard deviation\n",
    " - historgram of review length by parent company vs overall distribution\n",
    "     - summary statistics\n",
    " - correlation between review length and star ratings\n",
    "     - correlation coefficients\n",
    " - market share graphs\n",
    " - writing \n",
    "     \n",
    " Michelle:    \n",
    " - bar chart (bins of helpful votes) to see distribution of helpful votes\n",
    " - percentage of verified purchases, vine reviews by parent company\n",
    "     - correlation between verified purchases and helpful votes\n",
    "     - correlation between vine and helpful votes\n",
    " - distribution of vine, verified purchases, etc. ratings compare\n",
    " - markdown and cleaning of data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbda57aa",
   "metadata": {},
   "source": [
    "# Question for Reviewers\n",
    "1. Are we able to make predictions from observed trends in the Amazon beauty reviews? If so, what types of predictions will we be able to reliably make?\n",
    "2. Are there any other research areas that we can explore?\n",
    "3. Should we attempt to conduct natural language processing/sentiment analysis on the review text? Will that be beyond our capabilities as introductory students.\n",
    "4. Should we expand on into new product categories/corporations to get a more well-rounded overview of consumer review trends?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
